{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Clase 8: NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "rW9B4v-EoMl5"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAf71EhcfJWr"
      },
      "source": [
        "1.   [Scikit-learn/sklearn](https://scikit-learn.org/stable/)\n",
        "2.   [Tensorflow](https://www.tensorflow.org/?hl=es)\n",
        "3.   [Keras](https://keras.io/)\n",
        "4.   [Pytorch](https://pytorch.org/)\n",
        "5.   [Nltk](https://www.nltk.org/)\n",
        "6.   [Spacy](https://spacy.io/)\n",
        "\n",
        "Puede encontrar mas informacion y librerias utiles en [este blog](https://www.iartificial.net/librerias-de-python-para-machine-learning/#TensorFlow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHBuxc3Pi99u"
      },
      "source": [
        "Lecturas recomendadas:\n",
        "\n",
        "\n",
        "1.   [Word2vec](https://towardsdatascience.com/word2vec-research-paper-explained-205cb7eecc30)\n",
        "2.   [Transformers](https://compvis.github.io/taming-transformers/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rW9B4v-EoMl5"
      },
      "source": [
        "#Vectorizacion de palabras\n",
        "\n",
        "Para poder procesar una palabra es necesario convertirla en algun objeto matematico que permita hacer esto, la opcion mas sencilla y util son los vectores, estos se crean siguiendo el algoritmo Word2vec de forma que cada componente del vector es una caracteristica de lo que conceptualmente significa una palabra, los humanos podemos hacer esto de forma sencilla, simplemente asociar conceptos a palabras.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-aAm2oHqFj9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7aa62247-159c-4a2f-8734-ff17f295dff6"
      },
      "source": [
        "!pip install -U spacy\n",
        "!python -m spacy download es_core_news_lg\n",
        "import spacy\n",
        "import es_core_news_lg\n",
        "nlp = es_core_news_lg.load()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting es-core-news-lg==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_lg-3.4.0/es_core_news_lg-3.4.0-py3-none-any.whl (568.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 568.0 MB 8.7 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from es-core-news-lg==3.4.0) (3.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (3.0.7)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (2.4.4)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (1.0.3)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (4.64.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (1.9.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (3.0.10)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (1.0.8)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (0.4.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (0.6.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (2.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (4.1.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (21.3)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (8.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->es-core-news-lg==3.4.0) (2.0.1)\n",
            "Installing collected packages: es-core-news-lg\n",
            "Successfully installed es-core-news-lg-3.4.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_lg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee98ei6udqoF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "b6d7f5e1-4ac0-493a-cb19-dc0adba7aee2"
      },
      "source": [
        "#@title Vector de una palabra\n",
        "Palabra = \"Mujer\" #@param {type:\"string\"}\n",
        "print(nlp.vocab[Palabra].vector)\n",
        "nlp.vocab[Palabra].vector.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-1.6986   -0.47477   0.052872  1.6822   -2.1922   -0.33053  -2.8451\n",
            "  2.8542   -1.1272    0.3726   -1.1565   -1.4455   -0.026307 -0.21483\n",
            " -1.6063   -2.9019   -1.6247   -1.1644   -3.2589    0.054047 -2.3966\n",
            "  2.6162   -1.0554    2.887     0.89608   0.39347  -0.84909   1.9269\n",
            " -0.87403   1.4195   -0.17229  -3.1222    0.13586  -3.0757   -0.259\n",
            "  0.52147   1.0222   -0.95234  -2.5224   -2.2281   -0.34086   1.2405\n",
            "  0.41894  -0.24196   0.73332   0.507    -1.6326   -1.0542    0.68955\n",
            " -1.9894   -0.36714   2.9487    0.9443   -3.4951    0.061863  0.30056\n",
            " -1.2533   -3.1818    0.68235  -1.2787    3.8869    1.4787   -1.716\n",
            " -1.3402    0.17168  -0.37193  -1.1219    4.3357    1.4447    1.6047\n",
            "  1.8129    1.4436   -3.3065    0.35     -4.6366    0.93072  -1.999\n",
            " -1.9677   -1.5412   -0.66011   0.18278  -0.15808   0.99018  -0.1925\n",
            "  3.3131   -1.6629   -0.057501 -1.641    -2.6101    1.6045    3.3298\n",
            " -2.3918   -0.66816  -2.9066    2.1528   -2.5794    1.1919    0.57717\n",
            " -1.9721    0.018732  3.1997    1.385     4.5868    1.128    -0.89465\n",
            "  0.83833   1.8307   -0.17185   0.86195  -3.7014    0.43545  -0.32954\n",
            "  0.71553   0.80881   0.15126   1.2527   -1.1797    1.314    -1.8754\n",
            " -0.9992    2.5988    1.226     1.1908   -0.50893  -1.594    -1.1648\n",
            " -3.4409   -1.4003   -0.18153  -2.03     -2.412     0.18618   2.812\n",
            " -0.21011   2.2594    1.1768   -0.691    -4.6836   -0.81772   1.9406\n",
            "  3.5093   -1.281     1.5949   -0.21719   0.82439  -4.4568    1.5297\n",
            " -4.1205    2.195    -0.59392   1.5095    1.6935   -1.2697   -3.1998\n",
            " -0.84283  -0.89811  -0.6022   -1.8522   -0.88238   1.1462    2.2364\n",
            "  3.2219   -0.94058  -0.2861    1.2449    2.7067    1.3221    0.37118\n",
            "  1.346    -2.75     -2.4547    0.98451   1.2268    2.1669   -0.20282\n",
            "  1.9926    1.2265   -1.0884   -2.0859    0.33193  -3.0245    1.4619\n",
            "  0.35662  -1.5588    3.0741   -0.3853   -0.95863   3.0541   -1.9315\n",
            "  1.4541   -3.3029   -1.24     -0.97398  -3.4596    2.53      0.64103\n",
            "  3.2694   -1.2259    3.238     1.0862   -0.057258  1.4726   -1.6073\n",
            " -1.674     3.2869   -0.63794  -1.111    -0.040375  0.68569   1.1607\n",
            "  1.7777    0.26362   1.7491    0.24087   0.62636   3.1229    2.1109\n",
            " -1.6615    0.59012  -1.1      -0.30707  -2.227    -0.57322   0.91719\n",
            " -1.3846   -1.8746   -0.089147  0.67701  -1.388    -4.2599    1.3569\n",
            "  1.4766    0.31365  -3.2853    0.5939   -0.19097   2.3552    0.10794\n",
            " -1.5816    0.8947   -1.3877   -1.0866    0.30137   3.4079   -1.3184\n",
            "  1.2026   -0.15322  -2.8392   -1.8005    2.087    -0.99853   0.91025\n",
            "  2.6975   -0.045593 -1.0745   -0.15854   2.2293   -0.27502  -2.9057\n",
            " -0.80549   0.78149  -0.19579   0.61501   1.6113   -2.0188   -1.3064\n",
            "  0.39068  -0.83112   2.1488    2.963    -0.37788  -0.99821   2.0128\n",
            "  0.50715   1.124    -2.1066    2.674    -1.5775   -3.9159   -0.73208\n",
            " -0.35936  -2.031    -2.1564   -1.2524   -0.50238   0.4792   -1.4796\n",
            "  2.0655    0.69528   1.8388   -3.0212    0.098973 -1.6172   -2.348\n",
            " -2.916     0.87886  -0.22336  -0.87257   1.8515   -1.6564  ]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300,)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oXPs2r-qtLa"
      },
      "source": [
        "Ya que son vectores se pueden hacer operaciones matematicas, hay una operacion muy conocida:\n",
        "$$Rey-Hombre+Mujer=Reina$$\n",
        "Esta operacion resulta interesante ya que significa que hay una relacion conceptual entre diferentes tipos de palabras.\n",
        "\n",
        "tambien se puede hallar un parecido entre palabras:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A=nlp.vocab[\"Rey\"].vector-nlp.vocab[\"Hombre\"].vector+nlp.vocab[\"Mujer\"].vector\n",
        "B=nlp.vocab[\"Reina\"].vector\n",
        "import numpy as np\n",
        "print((A@B)/(np.linalg.norm(A)*np.linalg.norm(B)))"
      ],
      "metadata": {
        "id": "TdZfRU9X_Wek",
        "outputId": "1df7b982-2d42-4864-9891-656b9ca19f7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7030733\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "JkmFxVRTrEZE",
        "outputId": "cbd952d0-fdde-4251-8e53-eb7ae4d1d1eb"
      },
      "source": [
        "#@title Similitud entre dos palabras\n",
        "Palabra1 = \"reina\" #@param [\"hombre\", \"mujer\", \"rey\", \"reina\"] {allow-input: true}\n",
        "Palabra2= \"rey\" #@param [\"hombre\", \"mujer\", \"rey\", \"reina\"] {allow-input: true}\n",
        "\n",
        "print(nlp.vocab[Palabra1].similarity(nlp.vocab[Palabra2]))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6265181303024292\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bLPHboNsEqy"
      },
      "source": [
        "Incluso el mismo metodo puede determinar si dos frases u oraciones son similares o diferentes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKVakejesKEr",
        "cellView": "form",
        "outputId": "bc45cba9-f639-4f4e-9cc9-184c77660446"
      },
      "source": [
        "#@title Similitud entre dos frases u oraciones\n",
        "Palabra1 = \"El cielo es azul\" #@param [\"La similitud coseno es una forma de obtener informacion de cercania entre vectores\"] {allow-input: true}\n",
        "Palabra2 = \"Esta despejado el cielo\" #@param [\"La similitud coseno es una forma de conocer el angulo entre los vectores\"] {allow-input: true}\n",
        "\n",
        "p1 = nlp(Palabra1)\n",
        "p2 = nlp(Palabra2)\n",
        "p1.similarity(p2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5392067534075368"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTF31XakuKir"
      },
      "source": [
        "#Clasificacion de texto basica\n",
        "La clasificacion de texto es uno de los usos del procesamiento del lenguaje natural, es util para conocer el contenido de una oracion por ejemplo en un chatbot para conocer que pide el usuario o en un correo para determinar si es spam, hay otras aplicaciones que empiezan a juntar diferentes de estos modelos para formar modelos como los proyectos **Dall-e** y **VQGAN+CLIP**, este ultimo es un modelo que analiza el texto para generar una imagen e incluso se ha visto en **Github copilot** una IA que puede autocompletar codigo o generar codigo con comentarios."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRpXEpWpw29q"
      },
      "source": [
        "El clasificador que se usará es el mostrado por [Tensorflow](https://www.tensorflow.org/tutorials/keras/text_classification?hl=es) para clasificar frases como positivas o negativas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDIn6Q56uIKL"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdFNSUEdgasJ"
      },
      "source": [
        "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"aclImdb_v1\", url,\n",
        "                                    untar=True, cache_dir='.',\n",
        "                                    cache_subdir='')#untar se pone true si el archivo se tiene que descomprimir\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jptDwr52kRbj",
        "outputId": "c7aa04b9-8a75-4bbb-cfb5-27ed0e2176fa"
      },
      "source": [
        "train_dir = os.path.join(dataset_dir, 'train')\n",
        "print(os.listdir(train_dir))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['unsupBow.feat', 'pos', 'labeledBow.feat', 'urls_pos.txt', 'urls_neg.txt', 'neg', 'unsup', 'urls_unsup.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqWELB0IlRvU",
        "outputId": "3e1f1d58-d4cb-4d35-9833-cc109d96c0f2"
      },
      "source": [
        "sample_file = os.path.join(train_dir, 'pos/1181_9.txt')\n",
        "with open(sample_file) as f:\n",
        "  print(f.read())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rachel Griffiths writes and directs this award winning short film. A heartwarming story about coping with grief and cherishing the memory of those we've loved and lost. Although, only 15 minutes long, Griffiths manages to capture so much emotion and truth onto film in the short space of time. Bud Tingwell gives a touching performance as Will, a widower struggling to cope with his wife's death. Will is confronted by the harsh reality of loneliness and helplessness as he proceeds to take care of Ruth's pet cow, Tulip. The film displays the grief and responsibility one feels for those they have loved and lost. Good cinematography, great direction, and superbly acted. It will bring tears to all those who have lost a loved one, and survived.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fnkidrhmiHQ"
      },
      "source": [
        "remove_dir = os.path.join(train_dir, 'unsup')\n",
        "shutil.rmtree(remove_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3ugiljKnoDM",
        "outputId": "5341e12f-6945-47fe-f3de-2a35d9b10d00"
      },
      "source": [
        "batch_size = 32\n",
        "seed = 42\n",
        "\n",
        "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/train',\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    seed=seed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5V8DYiQSnpbf"
      },
      "source": [
        "for text_batch, label_batch in raw_train_ds.take(1):\n",
        "  for i in range(4):\n",
        "    print(\"Review\", text_batch.numpy()[i])\n",
        "    print(\"Label\", label_batch.numpy()[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUwryDOBnvUz",
        "outputId": "98643d7b-39ac-4741-dbc2-4f6a2d19f4cf"
      },
      "source": [
        "raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/train',\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset='validation',\n",
        "    seed=seed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZz316tNn0K-",
        "outputId": "55cb4aa6-4114-4a89-9cbc-19a9a8887716"
      },
      "source": [
        "raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/test',\n",
        "    batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUr_Z8IKn1xl"
      },
      "source": [
        "def custom_standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
        "  return tf.strings.regex_replace(stripped_html,\n",
        "                                  '[%s]' % re.escape(string.punctuation),#<- Re escape escapa los signos de puntuación de forma que por ejemplo un \\ será convertido a \\\\\n",
        "                                  '')#<- Elimina los signos de puntuación"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EV7tL0Eln3jD"
      },
      "source": [
        "max_features = 10000\n",
        "sequence_length = 250\n",
        "\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=max_features,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8NPMNqWn-oc"
      },
      "source": [
        "# Make a text-only dataset (without labels), then call adapt\n",
        "train_text = raw_train_ds.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(train_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RcA-HR7n_5o"
      },
      "source": [
        "def vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return vectorize_layer(text), label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9-6Hn0FoBrT"
      },
      "source": [
        "# retrieve a batch (of 32 reviews and labels) from the dataset\n",
        "text_batch, label_batch = next(iter(raw_train_ds))\n",
        "first_review, first_label = text_batch[0], label_batch[0]\n",
        "print(\"Review\", first_review)\n",
        "print(\"Label\", raw_train_ds.class_names[first_label])\n",
        "print(\"Vectorized review\", vectorize_text(first_review, first_label))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDZH_G8moJqQ"
      },
      "source": [
        "train_ds = raw_train_ds.map(vectorize_text)\n",
        "val_ds = raw_val_ds.map(vectorize_text)\n",
        "test_ds = raw_test_ds.map(vectorize_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ovwAdDpoMec"
      },
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE#<- Autotune precarga datos de entrenamiento y ajusta la cantidad para ahorrar tiempo\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMbgi0-3oPY7"
      },
      "source": [
        "embedding_dim = 16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW3bq0ygoR1R"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "  layers.Embedding(max_features + 1, embedding_dim),\n",
        "  layers.Dropout(0.2),\n",
        "  layers.GlobalAveragePooling1D(),\n",
        "  layers.Dropout(0.2),\n",
        "  layers.Dense(1)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXIctgKwoWDw"
      },
      "source": [
        "model.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer='adam',\n",
        "              metrics=tf.metrics.BinaryAccuracy(threshold=0.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZstcikgboXso",
        "outputId": "9dee6c41-d889-4984-80d6-413a55475edc"
      },
      "source": [
        "epochs = 20\n",
        "model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "625/625 [==============================] - 11s 12ms/step - loss: 0.6646 - binary_accuracy: 0.6926 - val_loss: 0.6162 - val_binary_accuracy: 0.7724\n",
            "Epoch 2/20\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.5501 - binary_accuracy: 0.8015 - val_loss: 0.4992 - val_binary_accuracy: 0.8208\n",
            "Epoch 3/20\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.4452 - binary_accuracy: 0.8438 - val_loss: 0.4206 - val_binary_accuracy: 0.8468\n",
            "Epoch 4/20\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.3792 - binary_accuracy: 0.8674 - val_loss: 0.3741 - val_binary_accuracy: 0.8608\n",
            "Epoch 5/20\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.3354 - binary_accuracy: 0.8795 - val_loss: 0.3449 - val_binary_accuracy: 0.8664\n",
            "Epoch 6/20\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.3056 - binary_accuracy: 0.8888 - val_loss: 0.3258 - val_binary_accuracy: 0.8716\n",
            "Epoch 7/20\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2807 - binary_accuracy: 0.8972 - val_loss: 0.3126 - val_binary_accuracy: 0.8738\n",
            "Epoch 8/20\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2624 - binary_accuracy: 0.9041 - val_loss: 0.3033 - val_binary_accuracy: 0.8768\n",
            "Epoch 9/20\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2450 - binary_accuracy: 0.9117 - val_loss: 0.2967 - val_binary_accuracy: 0.8776\n",
            "Epoch 10/20\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2316 - binary_accuracy: 0.9169 - val_loss: 0.2922 - val_binary_accuracy: 0.8790\n",
            "Epoch 11/20\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2183 - binary_accuracy: 0.9227 - val_loss: 0.2889 - val_binary_accuracy: 0.8802\n",
            "Epoch 12/20\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2077 - binary_accuracy: 0.9256 - val_loss: 0.2870 - val_binary_accuracy: 0.8822\n",
            "Epoch 13/20\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1971 - binary_accuracy: 0.9305 - val_loss: 0.2862 - val_binary_accuracy: 0.8808\n",
            "Epoch 14/20\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1867 - binary_accuracy: 0.9349 - val_loss: 0.2864 - val_binary_accuracy: 0.8818\n",
            "Epoch 15/20\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1780 - binary_accuracy: 0.9384 - val_loss: 0.2872 - val_binary_accuracy: 0.8824\n",
            "Epoch 16/20\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1705 - binary_accuracy: 0.9410 - val_loss: 0.2886 - val_binary_accuracy: 0.8838\n",
            "Epoch 17/20\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1632 - binary_accuracy: 0.9444 - val_loss: 0.2906 - val_binary_accuracy: 0.8838\n",
            "Epoch 18/20\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.1557 - binary_accuracy: 0.9480 - val_loss: 0.2933 - val_binary_accuracy: 0.8824\n",
            "Epoch 19/20\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1485 - binary_accuracy: 0.9503 - val_loss: 0.2961 - val_binary_accuracy: 0.8818\n",
            "Epoch 20/20\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1412 - binary_accuracy: 0.9531 - val_loss: 0.3000 - val_binary_accuracy: 0.8810\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7facfebaca90>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kl2navK0qclv"
      },
      "source": [
        "export_model = tf.keras.Sequential([\n",
        "  vectorize_layer,\n",
        "  model,\n",
        "  layers.Activation('sigmoid')\n",
        "])\n",
        "\n",
        "export_model.compile(\n",
        "    loss=losses.BinaryCrossentropy(from_logits=False), optimizer=\"adam\", metrics=['accuracy']\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "YdBSzuLJp8nv",
        "outputId": "127778cd-e6be-4624-9c1f-a1cbf346c3e9"
      },
      "source": [
        "#@title Evaluar el modelo { run: \"auto\", vertical-output: true }\n",
        "texto = \"Excelent movie\" #@param [\"Excelent movie\", \"Terrible...\"] {allow-input: true}\n",
        "\n",
        "export_model.predict([texto])[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.0378819], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIcXp5wAupkm"
      },
      "source": [
        "#Clasificacion de texto para obtener el contexto de una frase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbw20vfWyeRZ"
      },
      "source": [
        "Ejecutar la siguiente celda si no se tiene un json para hacer la ejecucion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvIEmzrsupLG"
      },
      "source": [
        "datos=open(\"./datos.json\",\"w\")\n",
        "datos.write(\"{\\\"contenido\\\":[{\\\"tag\\\":\\\"saludo\\\",\\\"patrones\\\":[\\\"Hola\\\",\\\"Buen dia\\\",\\\"Buenos dias\\\",\\\"Buenas tardes\\\"]},\"+\n",
        "            \"{\\\"tag\\\":\\\"despedida\\\",\\\"patrones\\\":[\\\"Adios\\\",\\\"Hasta luego\\\",\\\"Nos vemos\\\"]},\"+\n",
        "            \"{\\\"tag\\\":\\\"servicio\\\",\\\"patrones\\\":[\\\"¿que servicios ofrecen?\\\",\\\"¿que productos ofrecen\\\",\\\"¿que hacen ustedes?\\\",\\\"¿que productos venden?\\\",\\\"¿que venden?\\\",\\\"¿que hace la empresa?\\\",\\\"¿que tienen a la venta?\\\"]},\"+\n",
        "            \"{\\\"tag\\\":\\\"errores\\\",\\\"patrones\\\":[\\\"quisiera hacer una queja\\\",\\\"tengo problemas con esto\\\",\\\"no funciona\\\",\\\"hay problemas con esto\\\",\\\"podria comunicarme con alguien para solucionar mi problema\\\",\\\"tengo un problema\\\",\\\"aqui hay un error\\\",\\\"hay un fallo\\\",\\\"Me fallo\\\",\\\"tengo un error con esto\\\",\\\"tengo problemas\\\",\\\"quisiera quejarme por un problema\\\",\\\"tengo un inconveniente\\\",\\\"quisiera quejarme por un inconveniente\\\",\\\"quiero quejarme por un inconveniente\\\"]},\"+\n",
        "            \"{\\\"tag\\\":\\\"trabajo\\\",\\\"patrones\\\":[\\\"¿Hay alguna oferta de trabajo?\\\",\\\"¿quisiera trabajar?\\\",\\\"¿ofrecen trabajo?\\\",\\\"tiene ofertas de trabajo\\\",\\\"Tiene algun empleo disponible\\\",\\\"ofrece algun empleo\\\"]}]}\")\n",
        "datos.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDGmLr7ivnVS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "001d7318-d82c-40f0-8fac-b1041890234d"
      },
      "source": [
        "!pip install nltk\n",
        "!pip install tensorflow\n",
        "!pip install tflearn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.8.2+zzzcolab20220527125636)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (4.1.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.46.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.26.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (14.0.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.7)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tflearn\n",
            "  Downloading tflearn-0.5.0.tar.gz (107 kB)\n",
            "\u001b[K     |████████████████████████████████| 107 kB 4.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.21.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.15.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from tflearn) (7.1.2)\n",
            "Building wheels for collected packages: tflearn\n",
            "  Building wheel for tflearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tflearn: filename=tflearn-0.5.0-py3-none-any.whl size=127299 sha256=f48ec2b1b958bb32d9e2873d46d14918f1295d58eaa0238dcb73d2dfe55653f4\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/14/2e/1d8e28cc47a5a931a2fb82438c9e37ef9246cc6a3774520271\n",
            "Successfully built tflearn\n",
            "Installing collected packages: tflearn\n",
            "Successfully installed tflearn-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BOMpwBnvnxN"
      },
      "source": [
        "import nltk\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "import numpy as np\n",
        "import tflearn\n",
        "import tensorflow as tf\n",
        "import json\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrPGKhA-vo1Z",
        "outputId": "a828c6b2-4e8d-403d-bfc6-0ec14c2bf0ce"
      },
      "source": [
        "stemmer=LancasterStemmer()\n",
        "with open(\"datos.json\",encoding=\"utf-8\") as archivo:\n",
        "  datos=json.load(archivo)\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHrar4LPw3nH",
        "outputId": "175397eb-e36a-4d26-d74b-4d661c696b7b"
      },
      "source": [
        "palabras=[]\n",
        "tags=[]\n",
        "auxX=[]\n",
        "auxY=[]\n",
        "\n",
        "for contenido in datos[\"contenido\"]:\n",
        "  for patrones in contenido[\"patrones\"]:\n",
        "    auxPalabra=nltk.word_tokenize(patrones)\n",
        "    palabras.extend(auxPalabra)\n",
        "    auxX.append(auxPalabra)\n",
        "    auxY.append(contenido[\"tag\"])\n",
        "\n",
        "    if contenido[\"tag\"] not in tags:\n",
        "      tags.append(contenido[\"tag\"])\n",
        "\n",
        "palabras=[stemmer.stem(w.lower()) for w in palabras if (w!=\"?\" and w!=\"¿\")]\n",
        "palabras=sorted(list(set(palabras)))\n",
        "print(set(palabras))\n",
        "tags=sorted(tags)\n",
        "print(palabras)\n",
        "\n",
        "entrenamiento=[]\n",
        "salida=[]\n",
        "salidaVacia=[0 for _ in range(len(tags))]\n",
        "\n",
        "for x, documento in enumerate(auxX):\n",
        "  cub=[]\n",
        "  auxPalabra=[stemmer.stem(w.lower()) for w in documento]\n",
        "  for w in palabras:\n",
        "      cub.append(1) if w in auxPalabra else cub.append(0)\n",
        "  filaSalida=salidaVacia[:]\n",
        "  filaSalida[tags.index(auxY[x])]=1\n",
        "  entrenamiento.append(cub)\n",
        "  salida.append(filaSalida)\n",
        "entrenamiento=np.array(entrenamiento)\n",
        "salida=np.array(salida)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'hol', 'por', 'de', '¿hay', 'adio', 'tengo', 'comunicarm', 'tien', 'tard', 'empres', 'par', 'problema', 'mi', 'solucion', 'quej', 'inconveny', 'oferta', 'quejarm', 'trabajo', '¿quisiera', 'con', 'alguy', 'me', 'problem', 'nos', 'er', 'hac', 'empleo', 'servicio', 'vent', 'la', 'podr', 'aqu', 'quisier', 'quiero', 'esto', 'trabaj', 'ofrec', 'vend', 'hay', 'algun', 'un', 'producto', 'dia', '¿que', 'ofert', 'a', 'no', 'hast', 'fallo', 'funcion', 'bueno', 'ust', 'vemo', 'buena', '¿ofrecen', 'dispon', 'buen', 'luego'}\n",
            "['a', 'adio', 'algun', 'alguy', 'aqu', 'buen', 'buena', 'bueno', 'comunicarm', 'con', 'de', 'dia', 'dispon', 'empleo', 'empres', 'er', 'esto', 'fallo', 'funcion', 'hac', 'hast', 'hay', 'hol', 'inconveny', 'la', 'luego', 'me', 'mi', 'no', 'nos', 'ofert', 'oferta', 'ofrec', 'par', 'podr', 'por', 'problem', 'problema', 'producto', 'quej', 'quejarm', 'quiero', 'quisier', 'servicio', 'solucion', 'tard', 'tengo', 'tien', 'trabaj', 'trabajo', 'un', 'ust', 'vemo', 'vend', 'vent', '¿hay', '¿ofrecen', '¿que', '¿quisiera']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CGHFQgBxImr",
        "outputId": "c9199b3b-a96b-44a6-cead-35f2f4e0f70f"
      },
      "source": [
        "from tensorflow.python.framework import ops\n",
        "ops.reset_default_graph()\n",
        "\n",
        "red=tflearn.input_data(shape=[None,len(entrenamiento[0])])\n",
        "red=tflearn.fully_connected(red,15)\n",
        "red=tflearn.fully_connected(red,15)\n",
        "\n",
        "red=tflearn.fully_connected(red,len(salida[0]),activation=\"softmax\")\n",
        "\n",
        "red=tflearn.regression(red)\n",
        "modelo=tflearn.DNN(red)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tflearn/initializations.py:165: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkPDSTgVxgqT",
        "outputId": "08045c4b-1d70-4124-c2ac-36f28058c0a4"
      },
      "source": [
        "modelo.fit(entrenamiento,salida,n_epoch=1000,batch_size=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Step: 3999  | total loss: \u001b[1m\u001b[32m0.02312\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 1000 | loss: 0.02312 -- iter: 30/35\n",
            "Training Step: 4000  | total loss: \u001b[1m\u001b[32m0.02260\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 1000 | loss: 0.02260 -- iter: 35/35\n",
            "--\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "wEL9N6mHx5M7",
        "outputId": "92f2cf96-eab1-432b-baeb-c3dd77ff6897"
      },
      "source": [
        "#@title Prueba del modelo { run: \"auto\", vertical-output: true }\n",
        "texto = \"\" #@param {type:\"string\"}\n",
        "entrada=texto\n",
        "cub=[0 for _ in range(len(palabras))]\n",
        "entradaProcesada=nltk.word_tokenize(entrada)\n",
        "entradaProcesada=[stemmer.stem(palabra.lower()) for palabra in entradaProcesada]\n",
        "for palabraIndividual in entradaProcesada:\n",
        "  for i,palabra in enumerate(palabras):\n",
        "    if palabra==palabraIndividual:\n",
        "      cub[i]=1\n",
        "    else:\n",
        "      cub[i]=0\n",
        "resultados=modelo.predict([np.array(cub)])\n",
        "print(resultados)\n",
        "print(tags)\n",
        "resultIndice=np.argmax(resultados)\n",
        "tag=tags[resultIndice]\n",
        "print(tag)\n",
        "\n",
        "respuestas={\n",
        "    \"despedida\":\"adios\",\n",
        "    \"errores\":\"lo sentimos\",\n",
        "    \"saludo\":\"hola\",\n",
        "    \"servicio\":\"...\",\n",
        "    \"trabajo\":\".,.\"\n",
        "}\n",
        "print(respuestas.get(tag))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.10683201 0.46585268 0.25528044 0.03847834 0.13355651]]\n",
            "['despedida', 'errores', 'saludo', 'servicio', 'trabajo']\n",
            "errores\n",
            "lo sentimos\n"
          ]
        }
      ]
    }
  ]
}